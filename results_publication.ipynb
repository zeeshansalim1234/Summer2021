{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "results_publication.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "91KqzEZxMkjM"
      ],
      "authorship_tag": "ABX9TyNL9cH4S4bL01gwN7w7ODMS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeeshansalim1234/Summer2021/blob/main/results_publication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91KqzEZxMkjM"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMEfSdwqzspf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca18aea0-0cda-41cf-a4b8-cbe4cb14ec22"
      },
      "source": [
        "\n",
        "!pip install sentence-transformers\n",
        "!pip install tweepy\n",
        "!pip install bert-extractive-summarizer\n",
        "!pip install nltk\n",
        "!pip install google-cloud-vision"
      ],
      "execution_count": 798,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.0.16)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.26.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.26.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.62.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.26.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.7.4.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.16)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (5.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.7/dist-packages (2.4.2)\n",
            "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (21.0)\n",
            "Requirement already satisfied: proto-plus>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (1.19.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (1.53.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (2.26.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (1.34.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (1.39.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-vision) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (2021.5.30)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-vision) (1.24.3)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faqwzaaTzzMB"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from google.colab import drive\n",
        "import tweepy\n",
        "import re\n",
        "import os,io\n",
        "from google.cloud import vision\n",
        "from google.cloud.vision_v1 import types\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n"
      ],
      "execution_count": 799,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MMp6_9DU8h8"
      },
      "source": [
        "# Query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhDljTefU78E"
      },
      "source": [
        "#sample_texts=[\"play the record\",\"record the play\",\"play the video\"]\n",
        "sample_texts=[\"Three years later, the coffin was still full of Jello.\",\"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friends go.\",\"The person box was packed with jelly many dozens of moths later.\",\"Jello is tasty\"]\n",
        "corpus=sample_texts[1:]\n",
        "query=sample_texts[0]\n"
      ],
      "execution_count": 858,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqBtfP_2VCJU",
        "outputId": "ce9d0cbe-17f4-49ea-fedb-7800b0df7784"
      },
      "source": [
        "print(\"Query: \"+query+\"\\n\")\n",
        "for i in range(0,len(corpus)):\n",
        "  print(\"Corpus[\"+str(i)+\"]: \"+corpus[i])"
      ],
      "execution_count": 865,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Three years later, the coffin was still full of Jello.\n",
            "\n",
            "Corpus[0]: The fish dreamed of escaping the fishbowl and into the toilet where he saw his friends go.\n",
            "Corpus[1]: The person box was packed with jelly many dozens of moths later.\n",
            "Corpus[2]: Jello is tasty\n",
            "Corpus[3]: cancer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw1UlM0FKFeS"
      },
      "source": [
        "# SPECTRE (with cosine-similarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eTCDY7Dz4zx"
      },
      "source": [
        "model = SentenceTransformer('allenai-specter')   # loading specter model"
      ],
      "execution_count": 866,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2IRgfSYz-mK"
      },
      "source": [
        "corpus_embeddings=model.encode(corpus, convert_to_tensor=True)"
      ],
      "execution_count": 868,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBsSeab80jkK"
      },
      "source": [
        "\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)"
      ],
      "execution_count": 869,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmKE_Sl20tmn"
      },
      "source": [
        "search_hits = util.semantic_search(query_embedding, corpus_embeddings,top_k=10)\n",
        "search_hits = search_hits[0]"
      ],
      "execution_count": 870,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i5TDJiE0xNv",
        "outputId": "c18e05e1-b532-49dc-ed9e-6a1b0a2347ce"
      },
      "source": [
        "print(str(search_hits))"
      ],
      "execution_count": 871,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'corpus_id': 1, 'score': 0.8889927864074707}, {'corpus_id': 2, 'score': 0.8098993301391602}, {'corpus_id': 0, 'score': 0.8050113320350647}, {'corpus_id': 3, 'score': 0.7104994058609009}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qYN8JAoVuc0",
        "outputId": "fbdf31ba-faee-4309-dba4-ade8697b04ef"
      },
      "source": [
        "print(\"Result SPECTRE:\\n\")\n",
        "count=0\n",
        "for hit in search_hits:\n",
        "  print(str(1+count)+\") \"+corpus[hit['corpus_id']]+\"(\"+str(hit['score'])+\")\\n\")\n",
        "  count+=1\n",
        "  "
      ],
      "execution_count": 872,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result SPECTRE:\n",
            "\n",
            "1) The person box was packed with jelly many dozens of moths later.(0.8889927864074707)\n",
            "\n",
            "2) Jello is tasty(0.8098993301391602)\n",
            "\n",
            "3) The fish dreamed of escaping the fishbowl and into the toilet where he saw his friends go.(0.8050113320350647)\n",
            "\n",
            "4) cancer(0.7104994058609009)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMssqDcbJ6rz"
      },
      "source": [
        "# Base-BERT (with cosine-similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXaLCMDb4aEu"
      },
      "source": [
        "# BERT\n",
        "sentences=sample_texts\n",
        "BERT_model_name='sentence-transformers/bert-base-nli-mean-tokens'\n"
      ],
      "execution_count": 873,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWpwxCjWF9mt"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ],
      "execution_count": 874,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Boqdly8GFcU"
      },
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(BERT_model_name)\n",
        "BERT_model=AutoModel.from_pretrained(BERT_model_name)\n"
      ],
      "execution_count": 875,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4V1GHN-_aNN"
      },
      "source": [
        "tokens={'input_ids': [],'attention_mask': []}\n",
        "\n",
        "for sentence in sentences:\n",
        "  new_tokens=tokenizer.encode_plus(sentence,max_length=128,truncation=True,padding='max_length',return_tensors='pt')\n",
        "  tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "  tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "tokens['input_ids']=torch.stack(tokens['input_ids'])\n",
        "tokens['attention_mask']=torch.stack(tokens['attention_mask'])\n",
        "outputs=BERT_model(**tokens)\n",
        "embeddings=outputs.last_hidden_state\n",
        "attention=tokens['attention_mask']\n",
        "attention.shape\n",
        "mask=attention.unsqueeze(-1).expand(embeddings.shape).float()\n",
        "mask_embeddings=embeddings*mask\n",
        "summed=torch.sum(mask_embeddings,1)\n",
        "counts=torch.clamp(mask.sum(1),min=1e-9)\n",
        "mean_pooled=summed/counts"
      ],
      "execution_count": 876,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAEoTnfeJyWR"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "mean_pooled=mean_pooled.detach().numpy()\n",
        "result_bert=cosine_similarity([mean_pooled[0]],mean_pooled[1:])\n"
      ],
      "execution_count": 877,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF6GBL_HRs19",
        "outputId": "c98826bf-9e3f-4283-97e5-951799fb7f04"
      },
      "source": [
        "print(\"Result BERT(Cosine):\\n\")\n",
        "for i in range(0,len(result_bert[0])):\n",
        "  print(str(i+1)+\") \"+corpus[i]+\"(\"+str(result_bert[0][i])+\")\\n\")"
      ],
      "execution_count": 878,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result BERT(Cosine):\n",
            "\n",
            "1) The fish dreamed of escaping the fishbowl and into the toilet where he saw his friends go.(0.33093363)\n",
            "\n",
            "2) The person box was packed with jelly many dozens of moths later.(0.6258964)\n",
            "\n",
            "3) Jello is tasty(0.40863195)\n",
            "\n",
            "4) cancer(0.25527912)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arJJi8-HMjlA"
      },
      "source": [
        "# Base-BERT(with euclidean distance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB--z5v7M41g",
        "outputId": "4c83f0a4-3468-4c9c-b29c-eb11ea7bc5c7"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "result_bert=euclidean_distances([mean_pooled[0]],mean_pooled[1:])\n",
        "\n",
        "print(\"Result BERT(Euclidean):\\n\")\n",
        "for i in range(0,len(result_bert[0])):\n",
        "  print(str(i+1)+\") \"+corpus[i]+\"(\"+str(result_bert[0][i])+\")\\n\")"
      ],
      "execution_count": 879,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result BERT(Euclidean):\n",
            "\n",
            "1) The fish dreamed of escaping the fishbowl and into the toilet where he saw his friends go.(18.581127)\n",
            "\n",
            "2) The person box was packed with jelly many dozens of moths later.(13.716298)\n",
            "\n",
            "3) Jello is tasty(17.838797)\n",
            "\n",
            "4) cancer(20.03004)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah0quh3gg1pr"
      },
      "source": [
        "# TinyBERT (with cosine-similarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaPtQDW3Y5Br"
      },
      "source": [
        "# TinyBERT\n",
        "\n",
        "sentences=sample_texts\n",
        "TinyBERT_model_name='sentence-transformers/paraphrase-TinyBERT-L6-v2'\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(TinyBERT_model_name)\n",
        "TinyBERT_model=AutoModel.from_pretrained(TinyBERT_model_name)\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(TinyBERT_model_name)\n",
        "TinyBERT_model=AutoModel.from_pretrained(TinyBERT_model_name)\n",
        "\n",
        "tokens={'input_ids': [],'attention_mask': []}\n",
        "\n",
        "for sentence in sentences:\n",
        "  new_tokens=tokenizer.encode_plus(sentence,max_length=128,truncation=True,padding='max_length',return_tensors='pt')\n",
        "  tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "  tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "tokens['input_ids']=torch.stack(tokens['input_ids'])\n",
        "tokens['attention_mask']=torch.stack(tokens['attention_mask'])\n",
        "outputs=TinyBERT_model(**tokens)\n",
        "embeddings=outputs.last_hidden_state\n",
        "attention=tokens['attention_mask']\n",
        "attention.shape\n",
        "mask=attention.unsqueeze(-1).expand(embeddings.shape).float()\n",
        "mask_embeddings=embeddings*mask\n",
        "summed=torch.sum(mask_embeddings,1)\n",
        "counts=torch.clamp(mask.sum(1),min=1e-9)\n",
        "mean_pooled=summed/counts\n",
        "\n",
        "\n",
        "mean_pooled=mean_pooled.detach().numpy()\n",
        "result_tinybert=cosine_similarity([mean_pooled[0]],mean_pooled[1:])"
      ],
      "execution_count": 880,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX5Q2yS9r9lL",
        "outputId": "178a03a4-5c9c-430d-9616-f1369d1b4556"
      },
      "source": [
        "print(\"Result TinyBERT:\\n\")\n",
        "for i in range(0,len(result_tinybert[0])):\n",
        "  print(str(i+1)+\") \"+corpus[i]+\"(\"+str(result_tinybert[0][i])+\")\\n\")"
      ],
      "execution_count": 881,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result TinyBERT:\n",
            "\n",
            "1) The fish dreamed of escaping the fishbowl and into the toilet where he saw his friends go.(0.16096786)\n",
            "\n",
            "2) The person box was packed with jelly many dozens of moths later.(0.21702203)\n",
            "\n",
            "3) Jello is tasty(0.5115617)\n",
            "\n",
            "4) cancer(0.1285898)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSSxJBl6uE1i"
      },
      "source": [
        "# Count-Vectorizer(with euclidean distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVocpDv_uEVP"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "vectorizer=CountVectorizer()\n",
        "features=vectorizer.fit_transform(sample_texts).todense()\n",
        "vectorizer_results=[]\n",
        "for f in features:\n",
        "  vectorizer_results.append(euclidean_distances(features[0],f)[0][0])\n"
      ],
      "execution_count": 882,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAW245kiKnkp",
        "outputId": "ba1e69c4-8a56-4d40-c0ad-19798b1c9db5"
      },
      "source": [
        "print(\"Result CountVectorizer(Euclidean):\\n\")\n",
        "for i in range(1,len(vectorizer_results)):\n",
        "  print(str(i)+\") \"+sample_texts[i]+\"(\"+str(vectorizer_results[i])+\")\")"
      ],
      "execution_count": 883,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result CountVectorizer(Euclidean):\n",
            "\n",
            "1) The fish dreamed of escaping the fishbowl and into the toilet where he saw his friends go.(5.0)\n",
            "2) The person box was packed with jelly many dozens of moths later.(3.7416573867739413)\n",
            "3) Jello is tasty(3.3166247903554)\n",
            "4) cancer(3.3166247903554)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgiLDOr2l0js"
      },
      "source": [
        "# STS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCdYMagIlxCn",
        "outputId": "3e96bf4b-9d69-47ae-a5e2-7f38c1753359"
      },
      "source": [
        "#@title Load the Universal Sentence Encoder's TF Hub module\n",
        "from absl import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "\"\"\"\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "model = hub.load(module_url)\n",
        "\"\"\"\n",
        "model = SentenceTransformer('allenai-specter')   # loading specter model\n",
        "print (\"module %s loaded\" % module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": 947,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODnS-l9qlmL7"
      },
      "source": [
        "import pandas\n",
        "import scipy\n",
        "import math\n",
        "import csv\n",
        "\n",
        "sts_dataset = tf.keras.utils.get_file(\n",
        "    fname=\"Stsbenchmark.tar.gz\",\n",
        "    origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
        "    extract=True)\n",
        "sts_dev = pandas.read_table(\n",
        "    os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"),\n",
        "    error_bad_lines=False,\n",
        "    skip_blank_lines=True,\n",
        "    usecols=[4, 5, 6],\n",
        "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
        "sts_test = pandas.read_table(\n",
        "    os.path.join(\n",
        "        os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"),\n",
        "    error_bad_lines=False,\n",
        "    quoting=csv.QUOTE_NONE,\n",
        "    skip_blank_lines=True,\n",
        "    usecols=[4, 5, 6],\n",
        "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
        "# cleanup some NaN values in sts_dev\n",
        "sts_dev = sts_dev[[isinstance(s, str) for s in sts_dev['sent_2']]]"
      ],
      "execution_count": 948,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F6SvZw8l9z4"
      },
      "source": [
        "sts_data = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n",
        "\n",
        "def run_sts_benchmark(batch):\n",
        "  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)\n",
        "  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)\n",
        "  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
        "  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
        "  scores = 1.0 - tf.acos(clip_cosine_similarities) / math.pi\n",
        "  print(scores)\n",
        "  \"\"\"Returns the similarity scores\"\"\"\n",
        "  return scores\n",
        "\n",
        "count=0\n",
        "dev_scores = sts_data['sim'].tolist()\n",
        "scores = []\n",
        "for batch in np.array_split(sts_data, 10):\n",
        "  scores.extend(run_sts_benchmark(batch))\n",
        "  count+=1\n",
        "print(scores)\n",
        "print(dev_scores)\n",
        "\n",
        "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\n",
        "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
        "    pearson_correlation[0], pearson_correlation[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B6CzTgU5Bg4E",
        "outputId": "ca262256-a8a7-4786-f33e-7ff5f0084ea6"
      },
      "source": [
        "sts_data = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n",
        "\n",
        "def run_sts_benchmark(batch):\n",
        "  corpus_embeddings=model.encode(batch['sent_1'].tolist(), convert_to_tensor=True)\n",
        "  print(corpus_embeddings)\n",
        "  query_embedding=model.encode(batch['sent_2'].tolist(), convert_to_tensor=True)\n",
        "  cosine_similarities = util.cos_sim(corpus_embeddings,query_embedding)\n",
        "  \n",
        "  \"\"\"Returns the similarity scores\"\"\"\n",
        "  return scores\n",
        "\n",
        "count=0\n",
        "dev_scores = sts_data['sim'].tolist()\n",
        "scores = []\n",
        "for batch in np.array_split(sts_data, 10):\n",
        "  scores.extend(run_sts_benchmark(batch))\n",
        "  count+=1\n",
        "print(scores)\n",
        "print(dev_scores)\n",
        "\n",
        "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\n",
        "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
        "    pearson_correlation[0], pearson_correlation[1]))"
      ],
      "execution_count": 949,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.7240,  0.4943,  0.3280,  ...,  0.7217,  0.2838,  0.9984],\n",
            "        [-0.1791,  0.0862,  0.7585,  ...,  0.5637, -0.6136,  0.9263],\n",
            "        [-0.5648,  0.2241, -0.0707,  ...,  0.0960,  0.0409,  0.0091],\n",
            "        ...,\n",
            "        [-0.3696, -0.0465,  0.0101,  ...,  0.1096, -0.4859,  0.2426],\n",
            "        [-0.9151,  0.5695,  0.3251,  ...,  0.7222,  0.4850,  1.1786],\n",
            "        [-0.2413,  0.2298,  0.0778,  ...,  0.3689, -0.1202,  0.4072]])\n",
            "tensor([[-0.5603,  0.0867,  0.0911,  ..., -0.2811, -0.1409,  1.0207],\n",
            "        [-0.5730,  0.4517,  0.5408,  ...,  0.6669, -0.2083,  0.6010],\n",
            "        [-0.1315,  0.0501,  0.1877,  ...,  0.2441, -0.5864,  1.3464],\n",
            "        ...,\n",
            "        [-0.2551,  0.4869,  0.1325,  ..., -0.1725, -0.4059,  0.1329],\n",
            "        [-0.6723,  0.3509,  0.3281,  ...,  0.6964, -0.3846,  0.7508],\n",
            "        [-0.3340,  0.8520, -0.0315,  ...,  0.1320,  0.0378,  0.7145]])\n",
            "tensor([[-0.1982,  0.6213, -0.1159,  ...,  0.5121, -0.4424,  0.7374],\n",
            "        [-1.6208,  0.1651,  0.0928,  ...,  0.4666, -0.8313,  0.2309],\n",
            "        [-0.5358,  0.0459, -0.2769,  ..., -0.6462,  0.2288,  0.5294],\n",
            "        ...,\n",
            "        [ 0.0043,  0.5584,  0.0065,  ...,  0.4194, -0.6682,  0.6618],\n",
            "        [-0.5431, -0.0045,  0.3561,  ...,  0.2081, -0.5105,  1.4672],\n",
            "        [-0.4576,  0.5389,  0.7480,  ...,  0.5765, -0.5166,  0.8334]])\n",
            "tensor([[-0.4582,  0.3549,  0.7854,  ...,  0.1149, -0.7702,  1.3734],\n",
            "        [-0.3273,  0.0620,  0.4291,  ..., -0.4162, -0.6976,  1.3271],\n",
            "        [-0.3612, -0.0873,  0.2154,  ...,  0.2391, -1.3538,  1.5095],\n",
            "        ...,\n",
            "        [-0.0765,  1.3395,  0.6434,  ...,  0.8596, -0.6087,  0.4997],\n",
            "        [-0.3711,  0.5813,  0.6847,  ..., -0.3674,  0.3950,  0.1066],\n",
            "        [-0.7971,  0.3386,  0.2181,  ...,  0.7550, -0.1229,  1.5525]])\n",
            "tensor([[-0.4816, -0.3963,  0.0215,  ..., -0.3139,  0.0229,  0.9566],\n",
            "        [-1.1182, -0.4519, -0.9614,  ...,  0.1656,  0.0066,  1.4418],\n",
            "        [-0.5884,  0.1887, -0.1383,  ...,  0.2914, -0.1437,  0.4392],\n",
            "        ...,\n",
            "        [-0.3508, -0.0550,  0.5054,  ..., -0.4943, -0.3958,  0.1391],\n",
            "        [-0.2045,  0.7603, -0.3717,  ..., -0.2591,  0.3689,  0.6489],\n",
            "        [-0.8633,  0.5681, -0.0083,  ...,  0.4274,  0.2032,  0.0228]])\n",
            "tensor([[ 0.0655,  0.2293, -1.4434,  ..., -0.4381, -0.3067,  0.8167],\n",
            "        [-0.2235,  0.3785, -0.3879,  ..., -0.4084, -0.1214, -0.1321],\n",
            "        [-0.7074,  0.9676, -0.5456,  ..., -0.2122,  0.3087,  0.3314],\n",
            "        ...,\n",
            "        [ 0.0741,  0.1547, -0.5689,  ...,  0.9945, -1.0659,  0.5124],\n",
            "        [-0.8004, -0.0903, -0.5609,  ..., -0.3952, -0.4368,  1.4018],\n",
            "        [-0.7196, -0.1691,  0.0878,  ...,  0.0547, -0.3817,  0.4487]])\n",
            "tensor([[-0.8860,  0.2285,  0.3004,  ...,  0.6881, -0.4448,  0.8420],\n",
            "        [-0.1619,  0.5787, -0.3759,  ..., -0.1279,  0.3227,  1.0286],\n",
            "        [-0.5297,  0.6905, -0.4338,  ...,  0.2681, -0.0105,  0.3832],\n",
            "        ...,\n",
            "        [-0.4618, -0.1078,  0.0611,  ..., -0.0415,  0.1723,  1.1904],\n",
            "        [-0.9082,  0.7656, -0.4293,  ..., -0.3704, -0.2139,  0.6672],\n",
            "        [-1.0183,  0.6779,  0.2003,  ..., -0.3190,  0.8166,  0.2499]])\n",
            "tensor([[-0.9585,  0.3120, -0.2240,  ...,  0.2806, -0.9578,  0.9849],\n",
            "        [-0.2499,  0.0892, -0.5490,  ..., -0.2257, -0.0951,  1.5613],\n",
            "        [-0.0664, -0.5721,  0.7675,  ..., -0.1829, -0.5931,  0.6467],\n",
            "        ...,\n",
            "        [-0.5292, -0.4546,  0.1553,  ..., -0.1212, -0.8296,  1.1334],\n",
            "        [-0.4399,  0.6236, -0.1433,  ...,  0.9286, -0.3622,  0.3628],\n",
            "        [-0.6938,  0.3186,  0.4964,  ..., -0.1333, -0.5275,  1.2746]])\n",
            "tensor([[ 0.2755, -0.3821, -0.6103,  ...,  0.3948, -0.1070,  1.4386],\n",
            "        [ 0.0546,  0.9834,  0.0707,  ...,  0.3218,  0.0624, -0.0772],\n",
            "        [-1.1394, -0.0490, -1.1833,  ..., -0.9341, -0.9219,  0.7188],\n",
            "        ...,\n",
            "        [-0.6505, -0.6262, -0.3003,  ...,  0.8117,  0.1780,  1.2746],\n",
            "        [-0.1546, -0.0398,  0.0064,  ...,  0.5538, -0.1316,  0.2575],\n",
            "        [-0.3249,  0.4988, -0.9460,  ...,  0.5301, -0.9442,  0.9387]])\n",
            "tensor([[-0.5560,  0.9682,  0.4904,  ...,  0.7886, -0.4625,  0.1370],\n",
            "        [-0.2028,  0.4298, -0.6900,  ...,  0.3237,  0.3947,  1.6036],\n",
            "        [-0.4989,  0.9435, -1.0063,  ..., -0.1920, -0.8493,  0.6073],\n",
            "        ...,\n",
            "        [-0.3293, -0.8469,  0.6903,  ...,  0.5470, -0.4052,  0.6289],\n",
            "        [-0.5216,  0.4364, -1.1633,  ..., -0.2928, -0.1241,  1.8634],\n",
            "        [-0.4684,  0.8126, -0.2268,  ...,  0.3260, -0.4288,  0.6085]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-949-7946d9654023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_sts_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJLx6QUsCamN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}