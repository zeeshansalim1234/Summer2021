{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pachira_nan_reco.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZg94MzrspzRDxFfMUCGvD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeeshansalim1234/Summer2021/blob/main/Pachira_nan_reco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u4BGGhFHu2zA",
        "outputId": "0c74259d-874d-4630-eeb6-8f1fe4645124"
      },
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install tweepy\n",
        "!pip install bert-extractive-summarizer\n",
        "!pip install nltk\n",
        "!pip install google-cloud-vision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/fd/8a81047bbd9fa134a3f27e12937d2a487bd49d353a038916a5d7ed4e5543/sentence-transformers-2.0.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.4MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 49.7MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/35/03/071adc023c0a7e540cf4652fa9cad13ab32e6ae469bf0cc0262045244812/huggingface_hub-0.0.13-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-cp37-none-any.whl size=126711 sha256=9d9ab2b59955f470150fe49fab33eb8fbd4baf1a457b29a82bb9c46f2c639542\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/d2/98/d191289a877a34c68aa67e05179521e060f96394a3e9336be6\n",
            "Successfully built sentence-transformers\n",
            "\u001b[31mERROR: transformers 4.8.2 has requirement huggingface-hub==0.0.12, but you'll have huggingface-hub 0.0.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.0.13 sacremoses-0.0.45 sentence-transformers-2.0.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.8.2\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "Collecting bert-extractive-summarizer\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/07/fdb05f9e18b6f641499ef56737126fbd2fafe1cdc1a04ba069d5aa205901/bert_extractive_summarizer-0.7.1-py3-none-any.whl\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (4.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.45)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.10.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers->bert-extractive-summarizer) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->bert-extractive-summarizer) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
            "Installing collected packages: bert-extractive-summarizer, huggingface-hub\n",
            "  Found existing installation: huggingface-hub 0.0.13\n",
            "    Uninstalling huggingface-hub-0.0.13:\n",
            "      Successfully uninstalled huggingface-hub-0.0.13\n",
            "Successfully installed bert-extractive-summarizer-0.7.1 huggingface-hub-0.0.12\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting google-cloud-vision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/84/963344216d11e095995b20b55b5af3739db03439be7dfdf265de5271c714/google_cloud_vision-2.3.2-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 27.5MB/s \n",
            "\u001b[?25hCollecting proto-plus>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/72/6f3f4cdc5bb0294f8d7f3f8aacb617b4c3cb17554ed78f7e28009162c795/proto_plus-1.19.0-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (20.9)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (1.26.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from proto-plus>=1.15.0->google-cloud-vision) (3.12.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-vision) (2.4.7)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (57.0.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.31.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.53.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2018.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.34.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.7.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.10)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.4.8)\n",
            "Installing collected packages: proto-plus, google-cloud-vision\n",
            "Successfully installed google-cloud-vision-2.3.2 proto-plus-1.19.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BomJLNYUvBDJ"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from google.colab import drive\n",
        "import tweepy\n",
        "import re\n",
        "import os,io\n",
        "from google.cloud import vision\n",
        "from google.cloud.vision_v1 import types\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "ACCESS_TOKEN=\"1401579178072231936-98mY1wOw1UR3GHOjdo4ePnfdfUHt6n\"\n",
        "ACESS_TOKEN_SECRET=\"QiIslicgx8GNGrGDPrx8xNZc2WoCyrUsZumgXSyGzGLOx\"\n",
        "CONSUMER_KEY=\"uaiStuv7EYdYxWSomPKHvSSF5\"\n",
        "CONSUMER_SECRET=\"JOQryy37w9HPMSrSw8msyyb048iqeHmK4xCRyWP1oBhLKwLYlb\"\n",
        "\n",
        "auth=tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)\n",
        "auth.set_access_token(ACCESS_TOKEN,ACESS_TOKEN_SECRET)\n",
        "api=tweepy.API(auth)\n",
        "\n",
        "#os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'seismic-diorama-316110-5569927e0d86.json'\n",
        "#client = vision.ImageAnnotatorClient()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APH1nS4lvCOn"
      },
      "source": [
        "def pre_process(tweets):\n",
        "\n",
        "    for i in range(0, len(tweets)):\n",
        "\n",
        "        if (tweets[i] is not None):\n",
        "\n",
        "            if(tweets[i]!=tweets[i]):\n",
        "                tweets[i]=\"\"\n",
        "\n",
        "            tweets[i] = tweets[i].lower()  # To lower case\n",
        "            tweets[i] = tweets[i].replace('@','')  # remove @\n",
        "            tweets[i] = tweets[i].replace('#','')  # remove #\n",
        "            tweets[i] = remove_urls(tweets[i])  # remove URL\n",
        "            tweets[i] = remove_emojis(tweets[i])  # remove emojis\n",
        "            tweets[i] = \"\".join(j for j in tweets[i] if j not in (\n",
        "            \"?\", \".\", \";\", \":\", \"!\", \"-\", \",\", \"[\", \"]\", \"(\", \")\", \"’\", \"‘\", '\"', \"$\", \"'\", \"“\", \"”\", \"•\", \"=\", \"+\",\n",
        "            \"%\", \"/\", \"&\", \"|\", \"~\"))  # remove punctuations\n",
        "            tweets[i] = removeNonEnglishWordsFunct(tweets[i])\n",
        "\n",
        "    return tweets\n",
        "\n",
        "def remove_urls (str):\n",
        "\n",
        "    str = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', str, flags=re.MULTILINE)\n",
        "    return(str)\n",
        "\n",
        "\n",
        "def remove_emojis(data):\n",
        "\n",
        "    emoji = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoji, '', data)\n",
        "\n",
        "def removeNonEnglishWordsFunct(x):\n",
        "\n",
        "    new_string=re.sub('[^a-zA-Z0-9]',' ',x)\n",
        "\n",
        "    cleaned_string=re.sub('\\s+',' ',new_string)\n",
        "    return cleaned_string"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_Kp7NqBvCRO",
        "outputId": "4b2b5f4b-d46f-48c2-8db2-9ff76585c626"
      },
      "source": [
        "import numpy as np\n",
        "df = pd.read_csv('/content/charities_ab.csv')\n",
        "\n",
        "#charities =df.CharityName.values+\" \"+df.OngoingPrograms.values\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "charities=df.CharityName.values+\" \"+df.OngoingPrograms.values\n",
        "cities=df.City.values\n",
        "\n",
        "print(len(charities), \"Charities\")\n",
        "\n",
        "#We then load the allenai-specter model with SentenceTransformers\n",
        "model = SentenceTransformer('allenai-specter')\n",
        "\n",
        "#To encode the descriptions to a single string\n",
        "charity_texts = [charity for charity in charities]\n",
        "cities_texts=[city for city in cities]\n",
        "\n",
        "#Compute embeddings for all descriptions\n",
        "corpus_embeddings = model.encode(charity_texts, convert_to_tensor=True)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1562 Charities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyo5ylF-qBPl",
        "outputId": "9292d49a-c248-4c1d-ab01-31407728b3c6"
      },
      "source": [
        "print(charity_texts[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"POLANIE\" POLISH SONG & DANCE ASSOCIATION Performance for public: Concerts, Festivals, Private InvitesTeaching Polish folk songs and dance to people with polish roots and/or interest in polish cultureBall fundraising, casino fundraising, bake sale fundraising, mother day fundraising\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mgn_3eXvCTr"
      },
      "source": [
        "\n",
        "# Returns all charities at the provided location\n",
        "\n",
        "def search_papers_location(location):\n",
        "\n",
        "   results=[]\n",
        "   for i in range(0,len(cities_texts)):\n",
        "     if(cities_texts[i]==location):\n",
        "       results.append(i)\n",
        "   return results\n",
        "\n",
        "  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfFP0dtAvCWI",
        "outputId": "406ceca5-0a7c-424d-fb1a-d9b29c880c8a"
      },
      "source": [
        "# Generates 100 random charities as the priority\n",
        "\n",
        "import random\n",
        "from termcolor import colored\n",
        "\n",
        "#Generate 5 random numbers between 10 and 30\n",
        "priority = random.sample(range(0, 1520), 100)\n",
        "print(priority)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[677, 767, 552, 1406, 454, 37, 1440, 386, 486, 199, 1005, 95, 837, 316, 1462, 927, 643, 303, 27, 1280, 400, 54, 371, 443, 379, 679, 526, 147, 1055, 1263, 1108, 553, 1377, 634, 193, 556, 897, 521, 993, 134, 728, 23, 764, 745, 1387, 805, 321, 929, 42, 931, 1328, 423, 623, 684, 320, 1030, 122, 892, 133, 871, 1158, 8, 505, 294, 775, 1252, 1235, 1184, 315, 329, 1456, 390, 401, 178, 186, 879, 449, 441, 761, 641, 427, 826, 103, 461, 1492, 1471, 166, 380, 958, 1197, 1434, 473, 388, 757, 1476, 1321, 383, 359, 1267, 735]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnHkXC1nvCYv"
      },
      "source": [
        "# For generating recommendations for priority organizations\n",
        "\n",
        "def generate_priority(title):\n",
        "\n",
        "   query_embedding = model.encode(title+'[SEP]', convert_to_tensor=True) # Converts to tensor\n",
        "   search_hits = util.semantic_search(query_embedding, corpus_embeddings,top_k=10)\n",
        "   search_hits = search_hits[0]\n",
        "   \n",
        "   count = 0\n",
        "   top_header = \"\\n\\nPriority Charities\\n\\n\"\n",
        "   print(colored(top_header,'yellow'))\n",
        "\n",
        "   flag=0\n",
        "\n",
        "   for hit in search_hits:\n",
        "\n",
        "          for i in range(0,len(priority)):\n",
        "            if(hit['corpus_id']==priority[i]):\n",
        "              flag=1\n",
        "              break\n",
        "\n",
        "          if(flag==1 and search_hits[count]['score']>0.70):\n",
        "            related_charities = charities[hit['corpus_id']]\n",
        "            count += 1\n",
        "            subsetDataFrame = df[df['CharityName']+\" \"+df['OngoingPrograms']== related_charities]\n",
        "            k=subsetDataFrame.values\n",
        "\n",
        "            print(\"\\n\"+str(count)+\") \"+colored(str(k[0][0]), 'red'))\n",
        "            print(\"similiarity score of \" + str(format(search_hits[count-1]['score'],\".2f\")))\n",
        "            print(\"Description of charity : \" +related_charities+\" \"+cities_texts[hit['corpus_id']])\n",
        "            flag=0\n",
        "            count+=1\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxcKygNLvCbi"
      },
      "source": [
        "\n",
        "# For Generating recommendation for all organizations\n",
        "\n",
        "def search_papers(title,city_indices,status,num_results):\n",
        "\n",
        "   #status signifies city 0 is city based\n",
        "   #num_results signifies the number of results we wish to generate\n",
        "\n",
        "   query_embedding = model.encode(title+'[SEP]', convert_to_tensor=True) # Converts to tensor\n",
        "   search_hits = util.semantic_search(query_embedding, corpus_embeddings,top_k=10)\n",
        "   search_hits = search_hits[0]\n",
        "   \n",
        "   count = 0\n",
        "   flag=0\n",
        "   tracker=0 #basically counter for the non location segment\n",
        "\n",
        "\n",
        "\n",
        "   for hit in search_hits:\n",
        "        \n",
        "        if(status==0) :\n",
        "\n",
        "          for i in range(0,len(city_indices)):\n",
        "            if(hit['corpus_id']==city_indices[i]):\n",
        "              flag=1\n",
        "              break\n",
        "\n",
        "          if(flag==1):\n",
        "\n",
        "            related_charities = charities[hit['corpus_id']]\n",
        "            count += 1\n",
        "            subsetDataFrame = df[df['CharityName']+\" \"+df['OngoingPrograms']== related_charities]\n",
        "            k=subsetDataFrame.values\n",
        "            \n",
        "\n",
        "            print(\"\\n\"+str(count)+\") \"+colored(str(k[0][0]), 'red'))\n",
        "            print(\"similiarity score of \" + str(format(search_hits[count-1]['score'],\".2f\")))\n",
        "            print(\"Description of charity : \" +related_charities+\" \"+cities_texts[hit['corpus_id']])\n",
        "            flag=0\n",
        "\n",
        "        else:\n",
        "\n",
        "          if(tracker<num_results):\n",
        "            related_charities = charities[hit['corpus_id']]\n",
        "            count += 1\n",
        "            subsetDataFrame = df[df['CharityName']+\" \"+df['OngoingPrograms'] == related_charities]\n",
        "            k=subsetDataFrame.values\n",
        "            \n",
        "          \n",
        "            print(\"\\n\"+str(count)+\") \"+colored(str(k[0][0]), 'red'))\n",
        "            print(\"similiarity score of \" + str(format(search_hits[count-1]['score'],\".2f\")))\n",
        "            print(\"Description of charity : \" +related_charities+\" \"+cities_texts[hit['corpus_id']])\n",
        "            flag=0\n",
        "            tracker+=1\n",
        "\n",
        "\n",
        "#want to make a big impact donations made through birdies4kids for the alberta diabetes foundation are matched up to 50  birdies for kids runs until august 15th 2021  to donate and read more \n",
        "\n",
        "#alberta wins reddeer sisters donate land to nature conservancy of canada ncc preserve a haven for species wildlife ruth dorothy bower donate 193 hectares of land on the west bank of the reddeer river bowerwildlifesanctuary\n",
        "\n",
        "#in the news the kidney foundation encourages other canadian organizations and companies to to adopt policies to support living organdonors read more about kidneycanada s wage replacement policy for living organ and tissue donation\n",
        "\n",
        "#participate in the scotiabank calgary marathon charity challenge to help children amp families at childrens cottage society theres an event for everyone of all abilities register or donate at  thanks for your consideration children families \n",
        "\n",
        "#can you help us many children need their very own books strong reading role models amp safe fun reading spaces your donation helps more children experience the magic of reading amp a lifetime of opportunity "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UspOpeIy6WtZ",
        "outputId": "95863f26-3501-435e-b0f4-6db300b975da"
      },
      "source": [
        "pip install pytrends\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytrends\n",
            "  Downloading https://files.pythonhosted.org/packages/96/53/a4a74c33bfdbe1740183e00769377352072e64182913562daf9f5e4f1938/pytrends-4.7.3-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pytrends) (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytrends) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from pytrends) (1.1.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytrends) (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->pytrends) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.7.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhcEvvQ76YqP",
        "outputId": "77eb1a46-ded6-4f48-968f-8d70b4889cd0"
      },
      "source": [
        "from pytrends.request import TrendReq\n",
        "\n",
        "pytrends = TrendReq(hl='en-US') #specify language\n",
        "\n",
        "\"\"\"\n",
        "data=pytrends.trending_searches('canada')\n",
        "print(data[0].loc[0])\n",
        "\"\"\"\n",
        "\n",
        "pytrends.build_payload(kw_list=['protest'],geo='CA-AB',timeframe='now 7-d')  # generate the query\n",
        "related_queries = pytrends.related_queries()\n",
        "result=list(related_queries.values())\n",
        "query=list(result[0].values())[1]\n",
        "print(query)\n",
        "\n",
        "#search_papers(query,[],1,5)\n",
        "\n",
        "#list(result[0].values())[1].loc[0].at['query']"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          query  value\n",
            "0              winnipeg protest   1550\n",
            "1      tiananmen square protest    600\n",
            "2    no farmers no food protest    600\n",
            "3            canada day protest    190\n",
            "4          calgary mask protest    150\n",
            "5  edmonton legislature protest    130\n",
            "6                    gwen berry    110\n",
            "7     anti mask protest calgary     90\n",
            "8           residential schools     60\n",
            "9         farmers protest india     60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQthF28ovCeu",
        "outputId": "596acede-9a5e-4054-f970-33cc9a76fd92"
      },
      "source": [
        "print(\"What basis do you want to generate a recommendation on ? \")\n",
        "print(\"\\n1. Enter a sentence  \")\n",
        "print(\"2. Trending tweet on a particular hashtag \")\n",
        "print(\"3. Image-based recommendation generation \\n\")\n",
        "choice=input(\"Enter your choice : \")\n",
        "\n",
        "if(choice==\"1\"):\n",
        "\n",
        "  title=input(\"\\nEnter a sentence : \")\n",
        "  enable_loc=str(input(\"Do you wish to enable location ? \"))\n",
        "\n",
        "  pytrends = TrendReq(hl='en-US') #specify language\n",
        "  pytrends.build_payload(kw_list=['protest'],geo='CA-AB',timeframe=\"now 7-d\")  # generate the query\n",
        "  related_queries = pytrends.related_queries()\n",
        "  result=list(related_queries.values())\n",
        "  query=list(result[0].values())[1].loc[2].at['query']\n",
        "  print(\"\\n\"+query)\n",
        "  \n",
        "  top_header = \"\\n\\nPopular in your Region\\n\\n\"\n",
        "  print(colored(top_header,'magenta'))\n",
        "  search_papers(query,[],1,5)\n",
        "  \n",
        "\n",
        "  if(enable_loc==\"yes\"):\n",
        "\n",
        "    status=0\n",
        "    location=str(input(\"Enter location : \"))\n",
        "    loc_indices=search_papers_location(location)\n",
        "    generate_priority(title)\n",
        "    top_header = \"\\n\\nTop 10 related charities\\n\\n\"\n",
        "    print(colored(top_header,'blue'))\n",
        "    search_papers(title,loc_indices,status,10)\n",
        "\n",
        "  else:\n",
        "\n",
        "    pytrends = TrendReq(hl='en-US') #specify language\n",
        "    pytrends.build_payload(kw_list=['protest'],geo='CA-AB')  # generate the query\n",
        "    related_queries = pytrends.related_queries()\n",
        "    result=list(related_queries.values())\n",
        "    \n",
        "    temp=[]\n",
        "    status=1\n",
        "    generate_priority(title)\n",
        "    top_header = \"\\n\\nTop 10 related charities\\n\\n\"\n",
        "    print(colored(top_header,'blue'))\n",
        "    search_papers(title,temp,status,10)\n",
        "\n",
        "elif(choice==\"2\"):\n",
        "\n",
        "  tag = input(\"\\nEnter a hashtag : \")\n",
        "  tweets=tweepy.Cursor(api.search,q=tag,result_type='popular',tweet_mode=\"extended\").items(1)\n",
        "  temp=[]\n",
        "  for tweet in tweets :\n",
        "\n",
        "      temp.append(tweet.full_text)\n",
        "\n",
        "  temp=pre_process(temp)\n",
        "  tweet=temp[0]\n",
        "  print(\"\\nMost famous tweet : \\n\"+colored(tweet,'orange'))\n",
        "  search_papers(tweet)\n",
        "\n",
        "elif(choice==\"3\"):\n",
        "\n",
        "  with io.open(\"horse.jpg\", 'rb') as image_file:\n",
        "    content = image_file.read()\n",
        "\n",
        "\n",
        "  sample_tweet = \"\"\"Beautiful Rose (R) and beautiful Sunflower (L) are seen here enjoying a breezy afternoon here at the ranch. \n",
        "\n",
        "  Sparkleshttp://linktr.ee/PRRHR\n",
        "\n",
        "  #miniturehorses #horse #rescue #minihorsesoftwitter #BestFriendsDay #donate #tuesdayvibe\"\"\"\n",
        "\n",
        "  sample_tweet=pre_process([sample_tweet])\n",
        "\n",
        "  image = vision.Image(content=content)\n",
        "\n",
        "  response_label = client.label_detection(image=image)\n",
        "  response_text = client.text_detection(image=image)\n",
        "\n",
        "  temp=\"\"\n",
        "  count=0\n",
        "\n",
        "  for label in response_label.label_annotations:\n",
        "      if(count<3):\n",
        "        temp+=label.description+\" \"\n",
        "        count+=1\n",
        "\n",
        "  for r in response_text.text_annotations:\n",
        "      temp+=r.description+\" \"\n",
        "\n",
        "\n",
        "  temp=pre_process([temp])\n",
        "  print(\"\\n\\nInterpretation of tweet : \"+colored(sample_tweet[0]+\" \"+temp[0],'red'))\n",
        "  search_papers(sample_tweet[0]+\" \"+temp[0])\n",
        "\n",
        "\n",
        "\n",
        "#alberta wins reddeer sisters donate land to nature conservancy of canada ncc preserve a haven for species wildlife ruth dorothy bower donate 193 hectares of land on the west bank of the reddeer river bowerwildlifesanctuary\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What basis do you want to generate a recommendation on ? \n",
            "\n",
            "1. Enter a sentence  \n",
            "2. Trending tweet on a particular hashtag \n",
            "3. Image-based recommendation generation \n",
            "\n",
            "Enter your choice : 1\n",
            "\n",
            "Enter a sentence : alberta wins reddeer sisters donate land to nature conservancy of canada ncc preserve a haven for species wildlife ruth dorothy bower donate 193 hectares of land on the west bank of the reddeer river bowerwildlifesanctuary\n",
            "Do you wish to enable location ? no\n",
            "\n",
            "no farmers no food protest\n",
            "\u001b[35m\n",
            "\n",
            "Popular in your Region\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "1) \u001b[31mCold Lake Food Bank Society\u001b[0m\n",
            "similiarity score of 0.84\n",
            "Description of charity : Cold Lake Food Bank Society Food GivingMilk ProgramChristmas Hamper Project COLD LAKE\n",
            "\n",
            "2) \u001b[31mSOCIETY OF ST VINCENT DE PAUL ST BONVENTURE CONFERENCE CALGA\u001b[0m\n",
            "similiarity score of 0.81\n",
            "Description of charity : SOCIETY OF ST VINCENT DE PAUL ST BONVENTURE CONFERENCE CALGA WE RUN A FOOD BANK TO ASSIST NEEDY FAMILIES IN OUR COMMUNITY. WE ALSO HELP WITH RENT PAYMENTS AND UTILITY BILLS, MEDICAL PRESCRIPTIONS, BUS TICKETS AND MOVING EXPENSES. CALGARY\n",
            "\n",
            "3) \u001b[31mTHE BATTLE RIVER AGRICULTURAL SOCIETY\u001b[0m\n",
            "similiarity score of 0.81\n",
            "Description of charity : THE BATTLE RIVER AGRICULTURAL SOCIETY (1) PROMOTE AGRICULTURE, HOMEMAKING, HORTICULUTRE AND THE QUALITY OF LIFE IN THE COMMUNITY (2) MAINTAIN A COMMUNITY HALL, LIVESTOCK PAVILLION, CONCESSION BUILDING, MULTI-USE RIDING ARENA, BLEACHERS AND GROUNDS AROUND THESE FACILITIES. (3) SPONSOR A MAJOR EVENT (RODEO) AND OTHER MEETINGS, WORKSHOPS, 4H ACTIVITIES, FARMER'S MARKETS, HIGH SCHOOL RODEO AND MANY OTHER ACTIVITIES IN OUR FACILITIES (4) COOPERATE WITH OTHER COMMUNITY ORGANIZATIONS TO PROMOTE ACTIVITIES THAT BENEFIT OUR ENTIRE COMMUNITY. MANNING\n",
            "\n",
            "4) \u001b[31mCALGARY REGIONAL 4-H COUNCIL\u001b[0m\n",
            "similiarity score of 0.81\n",
            "Description of charity : CALGARY REGIONAL 4-H COUNCIL BURSARIES AND SCHOLARSHIPS, NEW CLUB START-UP FUNDING, MEMBER WORKSHOPS (MULTI-SPECIES JUDGING, CONSUMER DECISION MAKING, COMMUNICATIONS, VARIOUS SPECIES), MEMBER CLINICS, REGIONAL SHOWS - INCLUDING 4-H ON PARADE, MULTI-JUDGING EVENTS, COMMUNICATIONS COMPETITIONS, WINTER CAMP, AGE APPROPRIATE FUN DAYS, REGIONAL CURLING, LEADER DEVELOPMENT AND TRAINING, AWARDS FOR BOTH MEMBERS AND LEADERS, AWARDS CELEBRATION, CHARITABLE DONATIONS, COMMUNITY SERVICE, EQUIPMENT FUNDING, EXCHANGE PROGRAMS AIRDRIE\n",
            "\n",
            "5) \u001b[31mJEREMIAH ABEL MINISTRIES\u001b[0m\n",
            "similiarity score of 0.81\n",
            "Description of charity : JEREMIAH ABEL MINISTRIES MINISTERING FROM HOME EDMONTON\n",
            "\u001b[33m\n",
            "\n",
            "Priority Charities\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[34m\n",
            "\n",
            "Top 10 related charities\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "1) \u001b[31mCALGARY FIELD NATURALISTS' SOCIETY\u001b[0m\n",
            "similiarity score of 0.90\n",
            "Description of charity : CALGARY FIELD NATURALISTS' SOCIETY LEAD FIELD TRIPS TO LOCAL NATURAL AREAS AND PREPARE/MAINTAIN ONGOING SPECIES COUNTS. HOLD REGULAR MEETINGS TO EDUCATE PUBLIC AND MEMBERS ON NATURAL HISTORY MATTERS. FINANCIALLY SUPPORT SCHOOL CHILDREN TO VISIT LOCAL NATURAL AREAS TO GAIN AN APPRECIATION FOR NATURE AND TO LEARN ABOUT BIODIVERSITY AND ECOLOGICAL RELATIONSHIPS CALGARY\n",
            "\n",
            "2) \u001b[31mEdmonton and Area Land Trust\u001b[0m\n",
            "similiarity score of 0.88\n",
            "Description of charity : Edmonton and Area Land Trust THE EDMONTON AND AREA LAND TRUST CONTINUED ITS WORK IN CONSERVATION THROUGH SECURING, MONITORING, ANDSTEWARDING ECOLOGICALLY VALUABLE LAND, AND EDUCATING THE PUBLIC AND OTHER GROUPS ABOUT THE VALUES OFBIODIVERSITY AND CONSERVATION.  WE CONTINUED TO RECEIVE FINANCIAL DONATIONS AND GRANTS FOR THIS PURPOSE, AND TOPARTNER WITH OTHERS FOR THIS PURPOSE, AND TO WORK ON CONSERVATION EASEMENTS.  WE CONTINUED TO DEVELOPEDUCATION MATERIALS ABOUT ENVIRONMENTALLY SOUND STEWARDSHIP TARGETED TO VARIOUS AUDIENCES AND ON VARIOUSRELATED TOPICS, AS WELL AS TO MAKE PRESENTATIONS TO VARIOUS GROUPS, AND NO INVOLVE VOLUNTEERS IN OUR STEWARDSHIP,OUTREACH, EDUCATIONAL AND OTHER ACTIVITIES.  WE MAINTAIN 23KM OF TRAILS THROUGH OUR CONSERVATION LANDS, ASSISTEDBY MANY VOLUNTEERS.  WE CONTINUED OUR EARTH AMBASSADOR AWARD PROGRAM FOR BUSINESSES. EDMONTON\n",
            "\n",
            "3) \u001b[31mFriends of the Kerry Wood Nature Centre\u001b[0m\n",
            "similiarity score of 0.88\n",
            "Description of charity : Friends of the Kerry Wood Nature Centre OUR FUNDS PROVIDE SUPPORT FOR THE PURCHASING OF EQUIPMENT & SUPPLIES, GALLERY UPGRADES & LIBRARY RESOURCES THAT SUPPORT EDUCATIONAL & RECREATIONAL PROGRAM FOR CHILDREN, ADULTS, SENIORS IN ENVIRONMENTAL PROGRAMMING & NATURAL HISTORY IN ALBERTA. RED DEER\n",
            "\n",
            "4) \u001b[31mCROOKED CREEK CONSERVANCY SOCIETY OF ATHABASCA\u001b[0m\n",
            "similiarity score of 0.88\n",
            "Description of charity : CROOKED CREEK CONSERVANCY SOCIETY OF ATHABASCA WATERSHED MONITORING & ENVIRONMENTAL PROTECTION. CHRISTMAS BIRD COUNT.CROOKED CREEK WATERSHED STUDY EXTENSION. PRIVATE AND PUBLIC WILDLANDS CONSERVATIONAND LAND TRUST EDUCATION. ATHABASCA\n",
            "\n",
            "5) \u001b[31mPLATEAU PERSPECTIVES\u001b[0m\n",
            "similiarity score of 0.87\n",
            "Description of charity : PLATEAU PERSPECTIVES - BIODIVERSITY CONSERVATION THROUGH THE DEVELOPMENT OF DIGITAL FIELD GUIDES, FACILITATING LOCAL PARTICIPATION WITH CITIZEN SCIENCE.- PP STAFF CONTRIBUTING TO 'REGIONALIZATION PROCESS' OF THE ICCA CONSORTIUM WITH THE AIM TO STRENGTHEN MARGINALIZED COMMUNITIES.- DEVELOPMENT OF COMMUNITY ECOTOURISM, TRIALLED IN KYRGYZSTAN THROUGH PARTNERSHIP WITH HORSEBACK PLANET SOCIETY THAT BEGAN IN CHINA. LETHBRIDGE\n",
            "\n",
            "6) \u001b[31mWILLMORE WILDERNESS PRESERVATION AND HISTORICAL FOUNDATION\u001b[0m\n",
            "similiarity score of 0.87\n",
            "Description of charity : WILLMORE WILDERNESS PRESERVATION AND HISTORICAL FOUNDATION : Historical Research: Film Writing & Production: Trail Clearing & Restoration GRANDE CACHE\n",
            "\n",
            "7) \u001b[31mCALGARY CHAPTER PHEASANTS FOREVER CANADA SOCIETY\u001b[0m\n",
            "similiarity score of 0.87\n",
            "Description of charity : CALGARY CHAPTER PHEASANTS FOREVER CANADA SOCIETY 1.  RESTORATION OF WILDLIFE HABITAT AND CREATION OF NEW HABITAT THROUGH PLANTING TREES, SHRUBS AND GRASSES. PRESERVATION AND RESTORATION OF WETLANDS.2.  FENCING RIPARIAN AREAS TO PREVENT LIVESTOCK FROM DAMAGING STREAMS3.  EDUCATION PROGRAMS4.  PROMOTE PUBLIC AWARENESS OF PHEASANTS FOREVER AND IT'S PROGRAMS5.  MAINTENANCE OF HABITAT SITES6.  LAND PURCHASE PARTNERSHIPS IN SOUTHERN ALBERTA FOR THE PURPOSE OF RESTORING AND CREATING UPLAND WILDLIFE HABITAT7.  GOVERNMENT RELATIONS POLICY WORK CALGARY\n",
            "\n",
            "8) \u001b[31mROYAL TYRRELL MUSEUM COOPERATING SOCIETY\u001b[0m\n",
            "similiarity score of 0.87\n",
            "Description of charity : ROYAL TYRRELL MUSEUM COOPERATING SOCIETY Support research funding requests from the Royal Tyrrell Museum of Palaeontology including dinosaur research & palaeoecology, amphibian, mammal, specimen collection, preparation, visiting researcher, general research, in-house education, and distance learning programs. Also support funding requests for gallery exhibit design, development, installation and enhancement. DRUMHELLER\n",
            "\n",
            "9) \u001b[31mCOCHRANE ECOLOGICAL INSTITUTE COCHRANE WILDLIFE RESERVE SOCIETY\u001b[0m\n",
            "similiarity score of 0.86\n",
            "Description of charity : COCHRANE ECOLOGICAL INSTITUTE COCHRANE WILDLIFE RESERVE SOCIETY SWIFT FOX REHABILITATION - BREED THE SWIFT FOX AND RE-ESTABLISH IN THE WILDWHALE FORCE - TRACK WHALES AROUND THE WORLD, RESEARCH FOR MAINTAININ THE EARTH'S WATERS EDUCATION PROGRAMS - CHILDREN LEARN ABOUT THE NATURE AND THE DANGERS OF LOOSING ANY SPICIES UAV (DRONE) TO MONITOR WILDLIFE COCHRANE\n",
            "\n",
            "10) \u001b[31mDONALDA CEMETERY CLUB\u001b[0m\n",
            "similiarity score of 0.85\n",
            "Description of charity : DONALDA CEMETERY CLUB MAINTAIANCE: GRASS CUTTING, CLEANING FLOWER BEDS DONALDA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U69m84fwvSTh"
      },
      "source": [
        "\"\"\"\n",
        "from newsapi import NewsApiClient\n",
        "\n",
        "newsapi = NewsApiClient(api_key='90833a9050904d1c9cb011ddd785d357')\n",
        "\n",
        "# /v2/top-headlines\n",
        "top_headlines = newsapi.get_everything(q='global protest',sort_by='relevancy')\n",
        "print(top_headlines['articles'][5]['title'])\n",
        "search_papers(top_headlines['articles'][5]['title'],[],1)\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}